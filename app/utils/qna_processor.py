from typing import Dict, Any, List
import logging
import time
import uuid

from app.utils.document_processor import get_document_processor
from app.utils.openai_service import get_openai_service
from app.utils.pinecone_service import get_pinecone_service
from app.db import get_db

logger = logging.getLogger(__name__)


class QnAProcessor:
    """Process Q&A pairs and store in MongoDB + Pinecone (no file storage needed)"""
    
    def __init__(self):
        self.doc_processor = get_document_processor()
        self.openai_service = get_openai_service()
        self.pinecone_service = get_pinecone_service()
        self.db = get_db()
    
    async def process_qna_pairs(
        self,
        qna_pairs: List[Dict[str, str]],
        workspace_id: str,
        document_id: str
    ) -> Dict[str, Any]:
        """
        Process Q&A pairs: create chunks, embed, store in MongoDB + Pinecone
        
        Args:
            qna_pairs: List of Q&A pairs
            workspace_id: Workspace ID
            document_id: Document identifier (generated by route)
            
        Returns:
            Processing results
        """
        start_time = time.time()
        
        try:
            logger.info(f"Processing {len(qna_pairs)} Q&A pairs (ID: {document_id})")
            
            # 1. Create chunks from Q&A pairs
            chunks_data = self.doc_processor.process_qna(
                qna_pairs=qna_pairs,
                document_id=document_id,
                workspace_id=workspace_id
            )
            
            logger.info(f"Created {len(chunks_data)} chunks from Q&A pairs")
            
            # 2. Generate embeddings for all chunks
            texts = [chunk["text"] for chunk in chunks_data]
            embeddings = self.openai_service.generate_embeddings_batch(texts)
            
            logger.info(f"Generated {len(embeddings)} embeddings")
            
            # 3. Prepare metadata for Pinecone
            metadatas = [chunk["metadata"] for chunk in chunks_data]
            
            # 4. Store vectors in Pinecone
            pinecone_ids = self.pinecone_service.add_documents(
                texts=texts,
                metadatas=metadatas,
                namespace=workspace_id
            )
            
            logger.info(f"Stored {len(pinecone_ids)} vectors in Pinecone")
            
            # 5. Create document record in MongoDB (NO file storage for Q&A)
            document_record = self.db.create_document(
                document_id=document_id,
                workspace_id=workspace_id,
                document_type="qna",
                filename="qna_import.json",  # Virtual filename
                file_path=f"virtual://qna/{document_id}",  # Virtual path (no actual file)
                file_size=sum(len(pair.get("question", "") + pair.get("answer", "")) for pair in qna_pairs),
                total_chunks=len(chunks_data),
                file_metadata={
                    "source": "api_upload",
                    "total_pairs": len(qna_pairs),
                    "categories": list(set(pair.get("category", "General") for pair in qna_pairs))
                },
                processing_metadata={
                    "extraction_method": "direct_input",
                    "format": "qna_pairs"
                }
            )
            
            logger.info(f"Created Q&A document record in MongoDB")
            
            # 6. Store chunks in MongoDB
            mongo_chunks = []
            for i, chunk_data in enumerate(chunks_data):
                mongo_chunk = {
                    "chunk_id": chunk_data["id"],
                    "document_id": document_id,
                    "workspace_id": workspace_id,
                    "text": chunk_data["text"],
                    "pinecone_id": pinecone_ids[i],
                    "metadata": chunk_data["metadata"]
                }
                mongo_chunks.append(mongo_chunk)
            
            self.db.create_chunks_batch(mongo_chunks)
            
            logger.info(f"Stored {len(mongo_chunks)} chunks in MongoDB")
            
            # 7. Update user statistics
            user = self.db.get_user_by_workspace(workspace_id)
            if user:
                self.db.update_user_stats(
                    user_id=user["user_id"],
                    increment_documents=1,
                    increment_chunks=len(chunks_data)
                )
            
            processing_time = time.time() - start_time
            
            return {
                "document_id": document_id,
                "total_chunks": len(chunks_data),
                "processing_time": round(processing_time, 2)
            }
            
        except Exception as e:
            logger.error(f"Error processing Q&A pairs: {str(e)}")
            raise


# Singleton instance
_qna_processor = None

def get_qna_processor() -> QnAProcessor:
    """Get or create QnAProcessor singleton"""
    global _qna_processor
    if _qna_processor is None:
        _qna_processor = QnAProcessor()
    return _qna_processor